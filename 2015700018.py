import numpy as np
import matplotlib.pyplot as plt

__author__ = 'Yasin Almalioglu'


def create_hello_matrix(m, n):
    A = np.zeros((m, n))
    # H
    A[1:9, (1, 2, 5, 6)] = 1
    A[4:6, 3:5] = 1
    # E
    A[(2, 3, 5, 6, 8, 9), 9:15] = 1
    A[(4, 7), 9:11] = 1
    # L
    A[3:11, 17:19] = 1
    A[9:11, 19:23] = 1
    # L
    A[4:12, 25:27] = 1
    A[10:12, 27:31] = 1
    # O
    A[5:13, (33, 34, 37, 38)] = 1
    A[(5, 6, 11, 12), 35:37] = 1

    return A


# 9.3 a
m = 15
n = 40
A = create_hello_matrix(m, n)
plt.pcolormesh(np.flipud(A), cmap='gray')
plt.axis('tight')  # Just to remove extra space generated by pcolor
plt.title('Original matrix A')

# 9.3 b
U, s, V = np.linalg.svd(A, full_matrices=1)
print 'Singular values of A: \n', s

plt.figure()
plt.title('Singular values of A')
plt.xlabel('Index for singular values')
plt.ylabel('Singular values')
plt.plot(s)

math_rank_A = np.linalg.matrix_rank(A)
print 'Mathematically exact rank of A is: ', math_rank_A

# 9.3 c
plt.figure()
for i in range(0, math_rank_A):
    S = np.zeros((m, n))
    S[:i, :i] = np.diag(s[0:i])
    B = np.dot(U, np.dot(S, V))
    plt.subplot(5, 2, i + 1)
    plt.pcolormesh(np.flipud(B), cmap='gray')
    plt.title('Reconstructed matrix with rank: ' + str(i + 1))
    plt.axis('tight')  # Just to remove extra space generated by pcolor

# plt.show()

# 10.4 a
''' J is clock-wise rotation matrix. (Anti clock-wise version is: [c s; -s c]).
To interpret F easily, let's rewrite it as F = J*[-1 0; 0 1]. Thus, it is a reflection through y axis (e2) and
a clock-wise rotation. This itself can be also interpreted as a reflection through y axis(e2) rotated theta/2. '''


# 10.4 b
# QR factorization by Givens rotation


def calculate_givens(a, b):
    r = np.linalg.norm([a, b], 2)
    c_in = a / r
    s_in = b / r
    return c_in, s_in


m2 = 3  # number of rows for matrix in this question
n2 = 4  # number of columns for matrix in this question
D = np.random.rand(m2, n2)
print '\nOriginal matrix to be decomposed is: \n', D
for j in range(0, n2):  # Iterate over columns
    for i in range(m2 - 1, j, -1):  # Iterate over rows
        c, s = calculate_givens(D[i - 1, j], D[i, j])
        D[i - 1:i + 1, j] = np.dot(np.array([[c, s], [-s, c]]), D[i - 1:i + 1, j])

print '\nUpper triangular matrix generated by Givens rotation is: \n', D

# 10.4 c
''' Multiplication of 2x1 vector by J matrix requires 4 multiplications and 2 additions, 6 flops per row per column.
 Householder requires 4 flops per row per column as explained in the book. Thus, the Givens method requires 50% more
 than the Householder method. '''


# 11.3


def mgs(A):
    ''' Modified Gram-Schmidt orthogonalization of the
    matrix A = Q*R, where Q is orthogonal and R upper
    is triangular.
    :param A:
    :return: Q and R
    '''

    m, n = A.shape
    R = np.zeros((m, n))
    Q = np.zeros((m, m))
    for i in range(0, n):
        R[i, i] = np.linalg.norm(A[:, i])
        Q[:, i] = A[:, i] / R[i, i]
        for j in range(i + 1, n):
            R[i, j] = np.dot(Q[:, i].transpose(), A[:, j])
            A[:, j] = A[:, j] - R[i, j] * Q[:, i]
    return Q, R


# def house(A):
#     m, n = A.shape
#     R = A.copy()
#     Q = np.eye(m)
#     for k in range(0, n - 1):
#         x = R[k:m, k]
#         e = np.zeros((len(x), 1))
#         e[0] = 1
#         u = np.sign(x[0]) * np.linalg.norm(x) * e + x[:, np.newaxis]
#         u /= np.linalg.norm(u)
#
#         R[k:m, k:n] -= 2 * np.dot(u, np.dot(u.transpose(), R[k:m, k:n]))
#         Q[k:m, k, np.newaxis] = u
#
#     return Q, R

def house(A):
    m, n = A.shape
    Q = np.eye(m)
    for i in range(n - (m == n)):
        H = np.eye(m)
        H[i:, i:] = householder_reflector(A[i:, i])
        Q = np.dot(Q, H)
        A = np.dot(H, A)
    return Q, A


def householder_reflector(a):
    v = a / (a[0] + np.copysign(np.linalg.norm(a), a[0]))
    v[0] = 1
    H = np.eye(a.shape[0])
    H -= (2 / np.dot(v, v)) * np.dot(v[:, None], v[None, :])
    return H


m3 = 50
n3 = 12
t = np.linspace(0, 1, m3)
b = np.cos(4 * t)
E = np.fliplr(np.vander(t, n3))

# No scientific notation for small numbers. print is much better this way
np.set_printoptions(suppress=True)
np.set_printoptions(precision=16)

# Equivalent of MATLAB \ in Python is np.lstsq()
# ref: https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html
xa = np.linalg.lstsq(np.dot(E.transpose(), E), np.dot(E.transpose(), b))[0]
print '\nxa: \n', xa

Q3, R3 = mgs(E.copy())
xb = np.linalg.lstsq(R3, np.dot(Q3.transpose(), b))[0]
print '\nxb: \n', xb

Q4, R4 = house(E)
xc = np.linalg.lstsq(R4, np.dot(Q4.transpose(), b))[0]
print '\nxc: \n', xc

Q5, R5 = np.linalg.qr(E)
xd = np.linalg.lstsq(R5, np.dot(Q5.transpose(), b))[0]
print '\nxd: \n', xd

xe = np.linalg.lstsq(E, b)[0]
print '\nxe: \n', xe

U, s, V = np.linalg.svd(E, full_matrices=False)
S = np.diag(s)
xf = np.dot(V.transpose(), np.linalg.lstsq(S, np.dot(U.transpose(), b))[0])
print '\nxf: \n', xf

# 11.3 g
# Compare the residuals, Ex-b
# The results show that the last four methods yielded the best results.
# It seems it is a matter of choice, any of the four is winner (certainly not a and b though).
# The equations seem to be unstable.

print '\nResidual for part a: \n', np.linalg.norm(np.dot(E, xa) - b)
print '\nResidual for part b: \n', np.linalg.norm(np.dot(E, xb) - b)
print '\nResidual for part c: \n', np.linalg.norm(np.dot(E, xc) - b)
print '\nResidual for part d: \n', np.linalg.norm(np.dot(E, xd) - b)
print '\nResidual for part e: \n', np.linalg.norm(np.dot(E, xe) - b)
print '\nResidual for part f: \n', np.linalg.norm(np.dot(E, xf) - b)